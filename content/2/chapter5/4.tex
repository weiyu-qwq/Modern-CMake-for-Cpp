
优化器将分析前一阶段的输出，并使用大量的技巧，开发者可能会认为这些技巧很脏，因为它们不遵守干净代码的原则。这没关系——优化器的关键作用是提高代码的性能(使用较少的CPU周期、较少的寄存器和较少的内存)。当优化器遍历源代码时，将对其进行大量转换，以至于几乎无法阅读，会变成为目标CPU专门准备的版本。优化器不仅将决定哪些函数可以删除或压缩，还会移动代码，甚至复制代码!若能够完全确定某些代码行没有意义，其将从一个重要函数的中间删除(开发者不太会注意)。它将重用内存，因此许多变量可以在不同的时间段占用相同的插槽。若可以在这里或那里减少一些指令周期，它将把相应的控制结构转变成完全不同的结构。

这里描述的技术，若由程序员手动应用到源代码中，将会造成源代码变成可怕的、不可读的混乱符号集。很难阅读的同时，这些符号也很难写，也就更难于人为推演了。

另一方面，若编译器应用这些代码，其将完全按照所写的顺序进行。优化器是一头无情的野兽，只服务于一个目的:使执行快速，无论输出将如何混乱。若在测试环境中运行这样的输出，那么可能包含一些调试信息，也可能不包含，使未经授权的人难以修改。

每个编译器都有自己的技巧，与所遵循的平台和原理保持一致。我们将看一看最常见的，在GNU GCC和LLVM Clang中可用的优化器，从而了解什么是有用的，什么是可能的。

许多编译器默认情况下不会启用任何优化(包括GCC)。这在某些情况下是可以的，但在其他情况下就不是这样了。能快走为什么要慢走?要更改这些内容，可以使用target\_compile\_options()指令，并确切地指定想从编译器获得的内容。

该指令的语法与本章其他指令类似:

\begin{lstlisting}[style=styleCMake]
target_compile_options(<target> [BEFORE]
	<INTERFACE|PUBLIC|PRIVATE> [items1...]
	[<INTERFACE|PUBLIC|PRIVATE> [items2...] ...])
\end{lstlisting}  

我们提供要添加的目标命令行选项，并指定传播关键字。这个命令执行时，CMake将把给定的选项附加到目标的适当的COMPILE\_OPTIONS变量中。可选的BEFORE关键字可以用来指定把相应的选项放在前面。某些情况下，顺序很重要，这里可以选择，是件好事。

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=重要的Note]
target\_compile\_options()是一个通用指令。还可以用来为类似编译器的-D定义提供其他参数，为此CMake还提供了target\_compile\_definition()指令。总是建议尽可能使用CMake指令，因为其在所有支持的编译器上以相同的方式工作。
\end{tcolorbox}

是时候讨论细节了。后面的部分将介绍可以在大多数编译器中启用的各种优化。

\subsubsubsection{5.4.1\hspace{0.2cm}优化级别}

优化器的所有不同行为，都可以通过编译选项传递的特定标志进行配置。要了解所有这些方法挺花时间的，并且需要了解大量关于编译器、处理器和内存内部工作原理的知识。若只想要在大多数情况下运行良好的最佳可能场景，能做什么呢?可以找到一个通用的解决方案——优化级说明符。

大多数编译器提供了从0到3的四个基本优化级别，用-O<level>选项来指定它们。-O0表示不进行优化，通常这是编译器的默认级别。另一方面，-O2认为是完全优化，它生成高度优化的代码，但以最慢的编译时间为代价。

有一个介于两者之间的-O1级别，这(取决于需要)可能是一个很好的折衷方案——其支持合理数量的优化机制，而不会大幅降低编译速度。

最后，可以使用-O3，这是完全优化，就像-O2一样，但使用更积极的方法实现子程序内联和循环向量化。

还有一些优化变量会优化生成文件的大小(不一定是速度)，-Os。有一个超级激进的优化，-Ofast，不严格遵守C++标准的-O3优化。最明显的区别是- fast-math和-ffinite-math标志的使用，若程序是关于精确计算的(大多数都是这样)，最好望不使用这个参数。

CMake了解不是所有的编译器都是相同的。因此，其通过为编译器提供一些默认标志来标准化开发人员的体验。它们存储在所用语言(CXX用于C++)和构建配置(DEBUG或RELEASE)的系统范围(不是特定于目标的)变量中:

\begin{itemize}
\item 
CMAKE\_CXX\_FLAGS\_DEBUG 等于 -g

\item 
CMAKE\_CXX\_FLAGS\_RELEASE 等于 -O3 -DNDEBUG
\end{itemize}

As you can see, the debug configuration doesn't enable any optimizations and the release configuration goes straight for O3. If you like, you can change them directly with the set() command or just add a target compilation option, which will override this default behavior. The other two flags (-g, -DNDEBUG) are related to debugging – we'll discuss them in the Providing information for the debugger section.

Variables such as CMAKE\_<LANG>\_FLAGS\_<CONFIG> are global – they apply to all targets. It is recommended to configure your targets through properties and commands such as target\_compile\_options() rather than relying on global variables. This way, you can control your targets at higher granularity.

By choosing an optimization level with -O<level>, we indirectly set a long list of flags, each controlling a specific optimization behavior. We can then fine-tune the optimization by appending more flags, like so:

\begin{itemize}
\item 
Enable them with an -f option: -finline-functions.
	
\item 
Disable them with an -fno option: -fno-inline-functions.
\end{itemize}

Some of these flags are worth understanding better as they will often impact how your program works and how you can debug it. Let's take a look.

\subsubsubsection{5.4.2\hspace{0.2cm}函数内联}

As you will recall, compilers can be encouraged to inline some functions, either by defining a function inside a class declaration block or by explicitly using the inline keyword:

\begin{lstlisting}[style=styleCXX]
struct X {
	void im_inlined(){ cout << "hi\n"; };
	void me_too();
};
inline void X::me_too() { cout << "bye\n"; };
\end{lstlisting} 

It's up to the compiler to decide whether a function will be inlined. If inlining is enabled and the function is used in a single place (or is a relatively small function used in a few places), then inlining will most likely happen.

It's a really curious optimization technique. It works by extracting the code from the function in question and putting it in all the places the function was called, replacing the original call and saving precious CPU cycles.

Let's consider the following example using the class we just defined:

\begin{lstlisting}[style=styleCXX]
int main() {
	X x;
	x.im_inlined();
	x.me_too();
	return 0;
}
\end{lstlisting} 

Without inlining, the code would execute in the main() frame until a method call. Then, it would create a new frame for im\_inlined(), execute in a separate scope, and go back to the main() frame. The same would happen for the me\_too() method.

However, when inlining takes place, the compiler will replace the calls, like so:

\begin{lstlisting}[style=styleCXX]
int main() {
	X x;
	cout << "hi\n";
	cout << "bye\n";
	return 0;
}
\end{lstlisting} 

This isn't an exact representation because inlining happens at the level of assembly or machine code (and not the source code), but it conveys a general picture.
The compiler does it to save time; it won't have to go through the creation and teardown of a new call frame, it doesn't have to look up the address of the next instruction to execute (and return to), and it can cache the instructions better as they are nearby.

Of course, inlining has some important side effects; if the function is used more than once, it has to be copied to all places (meaning a bigger file size and more memory being used). Nowadays, this may not be so critical as it was in the past, but it's still relevant, as we constantly develop software that has to run on low-end devices without much RAM to spare.

Other than that, it affects us critically when we're debugging the code we wrote. Inlined code is no longer at the line number it was originally written, so it's not as easy (or sometimes even possible) to track. This is the exact reason why a debugger breakpoint placed in a function that was inlined never gets hit (although the code is still somehow executed). To avoid this issue, we simply have to disable inlining for debug builds (at the cost of not testing the exact same version as the release build).

We can do that by specifying the -O0 level for the target or going straight after the flags responsible:

\begin{itemize}
\item 
-finline-functions-called-once: GCC only

\item 
-finline-functions: Clang and GCC

\item 
-finline-hint-functions: Clang only

\item 
-finline-functions-called-once: GCC only
\end{itemize}

You can explicitly disable inlining with -fno-inline-.... In any case, for details, refer to the documentation of the specific version of your compiler.

\subsubsubsection{5.4.3\hspace{0.2cm}循环展开}

Loop unrolling is an optimization technique that is also known as loop unwinding. The general approach is to transform loops into a set of statements that achieve the same effect. By doing so, we'll trade the size of the program for execution speed, as we'll reduce or eliminate the instruction that controls the loop – pointer arithmetic or end-ofloop tests.

Consider the following example:

\begin{lstlisting}[style=styleCXX]
void func() {
	for(int i = 0; i < 3; i++)
	cout << "hello\n";
}
\end{lstlisting}

The previous code will be transformed into something like this:

\begin{lstlisting}[style=styleCXX]
void func() {
	cout << "hello\n";
	cout << "hello\n";
	cout << "hello\n";
}
\end{lstlisting}

The outcome will be the same, but we no longer have to allocate the i variable, increment it, or compare it three times with a value of 3. If we call func() enough times in the lifetime of the program, unrolling even such a short and small function will make a significant difference.

However, it is important to understand two limiting factors. Loop unrolling will only work if the compiler knows or can effectively estimate the amount of iterations. Secondly, loop unrolling can produce undesirable effects on modern CPUs, as increased code size might prevent effective caching.

Each compiler offers a slightly different version of this flag:

\begin{itemize}
\item 
-floop-unroll: GCC

\item 
-funroll-loops: Clang
\end{itemize}

If you're in doubt, test extensively whether this flag is affecting your particular program and explicitly enable or disable it. Do note that on GCC, it is implicitly enabled with -O3 as part of the implicitly enabled -floop-unroll-and-jam flag.

\subsubsubsection{5.4.4\hspace{0.2cm}循环向量化}

Single Instruction Multiple Data (SIMD) is one of the mechanisms developed in the early 1960s to achieve parallelism. It works exactly as the name suggests; it can perform the same operation on multiple pieces of information at the same time. What does it mean in practice? Let's consider the following example:

\begin{lstlisting}[style=styleCXX]
int a[128];
int b[128];
// initialize b
for (i = 0; i<128; i++)
	a[i] = b[i] + 5;
\end{lstlisting}

Normally, the preceding code would loop 128 times, but with a capable CPU, we can execute the code much faster by calculating two or more elements of the array at the same time. This works because there's no dependency between consecutive elements and no overlap of data between arrays. Smart compilers can transform the preceding loop into something similar to this (which happens on the assembly level):

\begin{lstlisting}[style=styleCXX]
for (i = 0; i<32; i+=4) {
	a[ i ] = b[ i ] + 5;
	a[i+1] = b[i+1] + 5;
	a[i+2] = b[i+2] + 5;
	a[i+3] = b[i+3] + 5;
}
\end{lstlisting}

GCC will enable such automatic vectorization of loops at -O3. Clang enables it by default. Both compilers offer different flags to enable/disable vectorization in particular:

\begin{itemize}
\item 
-ftree-vectorize -ftree-slp-vectorize to enable in GCC

\item 
-fno-vectorize -fno-slp-vectorize to disable in Clang (if things break)
\end{itemize}

The performance of vectorization comes from utilizing special instructions that CPU manufacturers provide, rather than just simply replacing the original form of the loop with the unrolled version. Therefore, it's not possible to achieve the same level of performance by doing it manually (also, it's not very clean code).

The role of the optimizer is important in enhancing the performance of the program during runtime. By employing its strategies effectively, we'll get more bang for our buck. Efficiency is important not only after the coding is completed but also as we work on the software. If the compilation times are lengthy, we can improve them by managing the process better.




